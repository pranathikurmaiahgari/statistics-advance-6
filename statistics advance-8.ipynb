{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff03000-8a95-4adb-9b1d-1e94a96cc6f1",
   "metadata": {},
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f89e41-0b38-4a99-874c-44bde1ad6849",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical method used to compare means across multiple groups. To ensure the validity of ANOVA results, certain assumptions must be met. These assumptions are:\n",
    "\n",
    "1. **Normality**: The data within each group should be approximately normally distributed. Violations of this assumption can impact ANOVA results, especially if sample sizes are small. For example, if the data are highly skewed or have heavy tails, it might be inappropriate to use ANOVA.\n",
    "\n",
    "2. **Homogeneity of Variances (Homoscedasticity)**: The variances within each group should be approximately equal. Homogeneity of variances ensures that the groups are roughly equivalent in terms of variability. Violations of this assumption, known as heteroscedasticity, can lead to increased Type I errors (false positives). If one group has significantly larger variances than others, it may affect the reliability of ANOVA results.\n",
    "\n",
    "3. **Independence of Observations**: Observations within each group must be independent of each other. Independence is a fundamental assumption in many statistical tests, and violations could lead to inaccurate standard errors and, consequently, incorrect p-values.\n",
    "\n",
    "4. **Random Sampling**: Data should be collected through a random sampling process. This assumption ensures that the sample is representative of the population and enhances the generalizability of the results.\n",
    "\n",
    "**Examples of Violations:**\n",
    "\n",
    "- **Non-Normality**: If the data within each group deviate significantly from a normal distribution, it may lead to inaccurate p-values and confidence intervals. For instance, if the data are highly skewed or exhibit heavy tails, ANOVA results might be less reliable.\n",
    "\n",
    "- **Heteroscedasticity**: Unequal variances between groups can impact the overall F-test in ANOVA. For example, if one group has much larger variability than others, it might contribute more to the overall variability, potentially leading to Type I errors.\n",
    "\n",
    "- **Correlated Observations**: If observations within groups are correlated, it violates the assumption of independence. For instance, if repeated measurements are taken on the same subjects over time, the observations may be correlated.\n",
    "\n",
    "- **Non-Random Sampling**: If the sampling process is not random, it may introduce biases into the sample, impacting the generalizability of the results.\n",
    "\n",
    "It's important to check these assumptions before interpreting the results of ANOVA. If assumptions are violated, alternative methods or transformations of the data may be considered, or a non-parametric test may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e984e6-1a19-4054-ac01-61fef94c59e6",
   "metadata": {},
   "source": [
    "Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9c133-6f41-454e-8b68-5a930f6f6898",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) comes in different types, and the choice of which type to use depends on the design of the study and the nature of the variables being analyzed. The three main types of ANOVA are:\n",
    "\n",
    "1. **One-Way ANOVA:**\n",
    "   - **Use Case:** When comparing means across two or more independent groups or levels of a single categorical variable.\n",
    "   - **Example:** Comparing the mean scores of students from different schools or the mean performance of individuals exposed to different treatments.\n",
    "\n",
    "2. **Two-Way ANOVA:**\n",
    "   - **Use Case:** When there are two independent categorical variables (factors) and you want to examine their main effects and the interaction between them.\n",
    "   - **Example:** Investigating the effects of both gender and treatment on exam scores. This involves two factors (gender and treatment) and their interaction.\n",
    "\n",
    "3. **Repeated Measures ANOVA:**\n",
    "   - **Use Case:** When measurements are taken on the same subjects at multiple points in time or under different conditions.\n",
    "   - **Example:** Assessing the impact of a drug on blood pressure levels measured before and after treatment in the same group of individuals.\n",
    "\n",
    "**When to Use Each Type:**\n",
    "- **Use One-Way ANOVA:**\n",
    "  - When comparing means across two or more independent groups or levels of a single categorical variable.\n",
    "  - Example: Comparing average scores of students from different schools.\n",
    "\n",
    "- **Use Two-Way ANOVA:**\n",
    "  - When there are two independent categorical variables, and you want to assess the main effects of each variable and their interaction.\n",
    "  - Example: Investigating the effects of both diet and exercise on weight loss. Here, diet and exercise are two factors, and the interaction term explores whether the combined effect is different from what would be expected by simply adding the individual effects.\n",
    "\n",
    "- **Use Repeated Measures ANOVA:**\n",
    "  - When measurements are taken on the same subjects under different conditions or at multiple time points.\n",
    "  - Example: Evaluating the impact of a drug on blood pressure levels measured before and after treatment in the same individuals.\n",
    "\n",
    "Choosing the right type of ANOVA is crucial for obtaining meaningful and accurate results, so it's essential to consider the study design and the nature of the variables being analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7d1ff-3ba1-4f53-8d74-4b8cf0949017",
   "metadata": {},
   "source": [
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3653db7-5957-48c7-86a0-6f96b1a31b72",
   "metadata": {},
   "source": [
    "The partitioning of variance in Analysis of Variance (ANOVA) refers to the division of the total variability observed in the data into different components, each associated with specific sources. Understanding this concept is crucial for gaining insights into the relative contributions of various factors or sources of variability in a study. ANOVA accomplishes this partitioning by distinguishing between different types of variance:\n",
    "\n",
    "1. **Total Variance (Total Sum of Squares - SST):**\n",
    "   - This represents the overall variability in the dependent variable (response variable) across all groups or conditions.\n",
    "\n",
    "2. **Between-Group Variance (Between-Group Sum of Squares - SSB):**\n",
    "   - This reflects the variability in the dependent variable that is attributable to the differences between the group means. It assesses whether the means of different groups are significantly different.\n",
    "\n",
    "3. **Within-Group Variance (Within-Group Sum of Squares - SSW):**\n",
    "   - Also known as the error variance, it represents the variability in the dependent variable that is not explained by the differences between group means. It reflects the random variability within each group.\n",
    "\n",
    "The partitioning of variance is typically summarized in an ANOVA table, which includes the degrees of freedom, sum of squares, mean squares, and the F-statistic. The F-statistic is calculated by dividing the between-group mean square by the within-group mean square.\n",
    "\n",
    "**Importance of Understanding Partitioning of Variance:**\n",
    "\n",
    "1. **Identifying Significant Sources of Variability:**\n",
    "   - By partitioning the total variability into between-group and within-group components, ANOVA helps identify whether the observed differences among group means are statistically significant.\n",
    "\n",
    "2. **Quantifying the Effect Size:**\n",
    "   - The proportion of total variance explained by between-group differences provides a measure of effect size. A larger proportion suggests a stronger effect.\n",
    "\n",
    "3. **Assessing the Model Fit:**\n",
    "   - Understanding how much of the total variance is explained by the model helps assess how well the model fits the data. A good model should account for a significant portion of the observed variability.\n",
    "\n",
    "4. **Interpreting F-Statistic:**\n",
    "   - The F-statistic, derived from the partitioned variances, is used to test the hypothesis that there are no significant differences between group means. Understanding the components of the F-statistic aids in interpreting the results of hypothesis tests.\n",
    "\n",
    "In summary, the partitioning of variance in ANOVA provides a structured way to analyze and interpret the sources of variability in a study, allowing researchers to draw meaningful conclusions about the effects of different factors on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54b5f7-9e9c-4492-937f-94d86e579648",
   "metadata": {},
   "source": [
    "Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b425c749-c9d8-43be-b741-ba842d3d2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 316.93333333333334\n",
      "Explained Sum of Squares (SSE): 68.13333333333334\n",
      "Residual Sum of Squares (SSR): 248.8\n",
      "F-statistic: 1.643086816720257\n",
      "P-value: 0.234042509471001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data for three groups\n",
    "group1 = [15, 18, 21, 24, 27]\n",
    "group2 = [12, 14, 18, 20, 22]\n",
    "group3 = [10, 13, 16, 19, 22]\n",
    "\n",
    "# Combine data from all groups\n",
    "all_data = np.concatenate([group1, group2, group3])\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate total sum of squares (SST)\n",
    "sst = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate group means\n",
    "group_means = [np.mean(group) for group in [group1, group2, group3]]\n",
    "\n",
    "# Calculate explained sum of squares (SSE)\n",
    "sse = np.sum([len(group) * (mean - overall_mean)**2 for group, mean in zip([group1, group2, group3], group_means)])\n",
    "\n",
    "# Calculate residual sum of squares (SSR)\n",
    "ssr = np.sum([(value - mean)**2 for group, mean in zip([group1, group2, group3], group_means) for value in group])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Sum of Squares (SST): {sst}\")\n",
    "print(f\"Explained Sum of Squares (SSE): {sse}\")\n",
    "print(f\"Residual Sum of Squares (SSR): {ssr}\")\n",
    "\n",
    "# Perform one-way ANOVA for comparison\n",
    "f_statistic, p_value = f_oneway(group1, group2, group3)\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c8734-e7af-4203-ba25-a83f66bbb267",
   "metadata": {},
   "source": [
    "Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b38c09-6d7e-4c87-974d-ee53004567eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows of Tooth Growth Dataset\n",
      "    len supp  dose\n",
      "0   4.2   VC   0.5\n",
      "1  11.5   VC   0.5\n",
      "2   7.3   VC   0.5\n",
      "3   5.8   VC   0.5\n",
      "4   6.4   VC   0.5\n",
      "\n",
      "==============================================================\n",
      "\n",
      "Main effects:\n",
      "C(supp)     205.350000\n",
      "C(dose)    2426.434333\n",
      "Name: sum_sq, dtype: float64\n",
      "\n",
      "==============================\n",
      "\n",
      "Interaction effect:\n",
      "C(supp):C(dose)    108.319\n",
      "Name: sum_sq, dtype: float64\n",
      "\n",
      "==============================\n",
      "\n",
      "ANOVA Table:\n",
      "                      sum_sq    df          F        PR(>F)\n",
      "C(supp)           205.350000   1.0  15.571979  2.311828e-04\n",
      "C(dose)          2426.434333   2.0  91.999965  4.046291e-18\n",
      "C(supp):C(dose)   108.319000   2.0   4.106991  2.186027e-02\n",
      "Residual          712.106000  54.0        NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Load the inbuilt dataset from statsmodels\n",
    "data = sm.datasets.get_rdataset(\"ToothGrowth\", \"datasets\").data\n",
    "\n",
    "# printing top 5 rows of Tooth Growth dataset\n",
    "print('Top 5 rows of Tooth Growth Dataset')\n",
    "print(data.head())\n",
    "print('\\n==============================================================\\n')\n",
    "\n",
    "# Define the model formula\n",
    "model_formula = \"len ~ C(supp) + C(dose) + C(supp):C(dose)\"\n",
    "\n",
    "# Fit the model using OLS regression\n",
    "model = ols(model_formula, data).fit()\n",
    "\n",
    "# Calculate the main effects and interaction effects\n",
    "main_effects = sm.stats.anova_lm(model, typ=2)['sum_sq'][:2]\n",
    "interaction_effect = sm.stats.anova_lm(model, typ=2)['sum_sq'][2:3]\n",
    "\n",
    "# Print the results\n",
    "print(\"Main effects:\")\n",
    "print(main_effects)\n",
    "print(\"\\n==============================\\n\")\n",
    "print(\"Interaction effect:\")\n",
    "print(interaction_effect)\n",
    "print(\"\\n==============================\\n\")\n",
    "print(\"ANOVA Table:\")\n",
    "print(sm.stats.anova_lm(model, typ=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc2f5c-aa80-43ab-804d-e7fde783ee57",
   "metadata": {},
   "source": [
    "Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482afcd6-b21f-4722-8984-25c986980ed5",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic is used to test whether there are significant differences between the means of three or more independent (unrelated) groups. The associated p-value helps determine the statistical significance of the observed differences. Here's how you can interpret the results:\n",
    "\n",
    "1. **F-Statistic:**\n",
    "   - In your case, the F-statistic is 5.23. This value represents the ratio of the variability between group means to the variability within groups. A higher F-statistic suggests that there may be significant differences among the group means.\n",
    "\n",
    "2. **P-value:**\n",
    "   - The p-value associated with the F-statistic is 0.02. This p-value is the probability of observing an F-statistic as extreme as the one obtained, assuming that there are no true differences between the group means.\n",
    "\n",
    "3. **Interpretation:**\n",
    "   - Since the p-value (0.02) is less than the commonly used significance level of 0.05, you would reject the null hypothesis. The null hypothesis in this context is that there are no significant differences between the group means. Therefore, you have evidence to suggest that there are indeed significant differences.\n",
    "\n",
    "4. **Conclusion:**\n",
    "   - Based on the results, you can conclude that there are statistically significant differences between at least two of the groups. However, the ANOVA itself does not identify which specific groups are different from each other; it only indicates that not all group means are equal.\n",
    "\n",
    "5. **Post-hoc Tests (if applicable):**\n",
    "   - If your ANOVA indicates significant differences, it is often followed by post-hoc tests (e.g., Tukey's HSD, Bonferroni correction) to identify which specific groups differ from each other.\n",
    "\n",
    "In summary, with an F-statistic of 5.23 and a p-value of 0.02, you would reject the null hypothesis and conclude that there are significant differences between at least two of the groups. Further analyses or post-hoc tests may be needed to determine the nature of these differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee2ab5-0927-4418-bcfc-80faefcbf6d6",
   "metadata": {},
   "source": [
    "Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e1772-4f19-4ff6-9370-3cb30ee41b54",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is crucial to ensure the validity and reliability of the results. The choice of method for handling missing data can impact the accuracy of the analysis. Here are some common approaches and their potential consequences:\n",
    "\n",
    "1. **Complete Case Analysis (Listwise Deletion):**\n",
    "   - **Method:** Exclude cases with missing data on any variable involved in the analysis.\n",
    "   - **Consequences:**\n",
    "     - Reduces sample size, potentially leading to loss of statistical power.\n",
    "     - Results may be biased if the missing data are not missing completely at random (MCAR). If the missingness is related to the outcome, it can introduce bias.\n",
    "\n",
    "2. **Pairwise Deletion (Available Case Analysis):**\n",
    "   - **Method:** Analyze all cases that have data for the specific comparison being made.\n",
    "   - **Consequences:**\n",
    "     - Preserves more data than complete case analysis.\n",
    "     - May introduce bias if the missing data are related to the outcome or if the pattern of missingness is not random.\n",
    "\n",
    "3. **Imputation Methods (e.g., Mean Imputation, Last Observation Carried Forward):**\n",
    "   - **Method:** Replace missing values with estimated values based on observed data.\n",
    "   - **Consequences:**\n",
    "     - Preserves sample size but can introduce bias if the imputation method is not appropriate.\n",
    "     - Mean imputation assumes missing values are missing completely at random, and it may underestimate variability.\n",
    "     - Last Observation Carried Forward assumes that missing values are stable over time, which may not be valid.\n",
    "\n",
    "4. **Multiple Imputation:**\n",
    "   - **Method:** Generate multiple datasets with imputed values and perform analyses on each dataset, then combine results.\n",
    "   - **Consequences:**\n",
    "     - Preserves sample size and provides more realistic estimates of uncertainty.\n",
    "     - Requires assumptions about the missing data mechanism, and inappropriate assumptions can still lead to biased results.\n",
    "\n",
    "5. **Maximum Likelihood Estimation (MLE):**\n",
    "   - **Method:** Estimates model parameters while accounting for missing data by maximizing the likelihood function.\n",
    "   - **Consequences:**\n",
    "     - Preserves sample size and provides unbiased parameter estimates under the assumption that data are missing at random (MAR).\n",
    "     - Assumes that the missing data mechanism is ignorable, and results can be biased if this assumption is violated.\n",
    "\n",
    "**General Considerations:**\n",
    "- The choice of method depends on the nature of missing data and the assumptions that can be reasonably made about the missing data mechanism.\n",
    "- It is essential to perform sensitivity analyses to assess the impact of different missing data handling methods on results.\n",
    "\n",
    "In summary, the consequences of using different methods for handling missing data in repeated measures ANOVA include potential biases, loss of statistical power, and the impact on the validity of the results. Researchers should carefully consider the missing data mechanism and choose an appropriate method based on the assumptions and goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc091a0-5bbc-4585-890c-163f48153806",
   "metadata": {},
   "source": [
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d451797-b4a5-4645-b8c7-32dad6fb4119",
   "metadata": {},
   "source": [
    "Post-hoc tests are used after ANOVA to further investigate pairwise differences between groups when the overall ANOVA test indicates significant differences. These tests help identify which specific groups differ from each other. Some common post-hoc tests include:\n",
    "\n",
    "1. **Tukey's Honestly Significant Difference (HSD):**\n",
    "   - **Use Case:** Tukey's HSD is used when you have three or more groups, and you want to identify which specific pairs of groups have significantly different means. It controls the familywise error rate, making it suitable for multiple comparisons.\n",
    "   - **Example:** In a study comparing the effectiveness of three different teaching methods, Tukey's HSD can be applied to determine which pairs of teaching methods lead to significantly different student performance.\n",
    "\n",
    "2. **Bonferroni Correction:**\n",
    "   - **Use Case:** Bonferroni correction is a conservative approach used to control the familywise error rate by adjusting the significance level for each pairwise comparison. It is suitable when you have a predetermined significance level and want to perform multiple comparisons without increasing the overall Type I error rate.\n",
    "   - **Example:** In a clinical trial comparing the efficacy of four different drug treatments, Bonferroni correction can be applied to assess pairwise differences while controlling for the increased risk of Type I errors.\n",
    "\n",
    "3. **Sidak Correction:**\n",
    "   - **Use Case:** Similar to Bonferroni correction, Sidak correction is used to control the familywise error rate. It is less conservative than Bonferroni and may be preferable when performing a large number of comparisons.\n",
    "   - **Example:** In a market research study comparing the mean ratings of several products across different demographics, Sidak correction can be applied to identify significant differences while controlling for familywise error.\n",
    "\n",
    "4. **Duncan's Multiple Range Test:**\n",
    "   - **Use Case:** Duncan's test is used when you have three or more groups and want to identify homogeneous subsets of means, meaning groups that do not differ significantly from each other.\n",
    "   - **Example:** In an agricultural study comparing the yields of different fertilizer treatments, Duncan's test can be used to group fertilizers that lead to similar yields.\n",
    "\n",
    "5. **Holm's Method:**\n",
    "   - **Use Case:** Holm's method is a step-down procedure that controls the familywise error rate. It is less conservative than Bonferroni and more powerful when there are substantial differences between group means.\n",
    "   - **Example:** In a marketing study comparing the sales performance of products in different regions, Holm's method can be applied to identify significant differences while adjusting for multiple comparisons.\n",
    "\n",
    "**Example Scenario:**\n",
    "Suppose you conducted a one-way ANOVA to compare the mean scores of students who received different types of training programs (A, B, C, and D). The ANOVA indicates a significant overall difference. A post-hoc test, such as Tukey's HSD, could be used to determine which specific pairs of training programs have significantly different mean scores.\n",
    "\n",
    "In summary, the choice of post-hoc test depends on factors such as the number of groups, the desired control of Type I errors, and the assumptions about the data. Researchers should carefully consider these factors to select an appropriate post-hoc test for their specific study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26a503-5bf0-4674-a711-4f2e98a807ca",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91d6013-ac54-45d3-af40-e954fa3e3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 21.809565795751926\n",
      "P-value: 5.0767681760454e-09\n",
      "There is a significant difference in mean weight loss between at least two diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Generate example data for the three diets\n",
    "np.random.seed(42)  # for reproducibility\n",
    "weight_loss_a = np.random.normal(loc=5, scale=2, size=50)\n",
    "weight_loss_b = np.random.normal(loc=7, scale=2, size=50)\n",
    "weight_loss_c = np.random.normal(loc=6, scale=2, size=50)\n",
    "\n",
    "# Concatenate the data\n",
    "weight_loss_data = np.concatenate([weight_loss_a, weight_loss_b, weight_loss_c])\n",
    "\n",
    "# Create corresponding group labels\n",
    "group_labels = ['A'] * 50 + ['B'] * 50 + ['C'] * 50\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(weight_loss_a, weight_loss_b, weight_loss_c)\n",
    "\n",
    "# Print the results\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference in mean weight loss between at least two diets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in mean weight loss between the diets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef09ef-d6df-446c-bc6c-71de4c4325c6",
   "metadata": {},
   "source": [
    "Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "interaction effects between the software programs and employee experience level (novice vs.\n",
    "experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9735160c-edcc-46ea-8153-5daa4f6f15ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA Table:\n",
      "                              sum_sq    df         F    PR(>F)\n",
      "C(Program)                  1.334021   2.0  0.193670  0.824297\n",
      "C(Experience)               5.096305   1.0  1.479736  0.227223\n",
      "C(Program):C(Experience)    8.396750   2.0  1.219018  0.300694\n",
      "Residual                  289.301266  84.0       NaN       NaN\n",
      "\n",
      "Interpretation:\n",
      "There is no significant main effect of software program on completion time.\n",
      "There is no significant main effect of experience level on completion time.\n",
      "There is no significant interaction effect between software program and experience level.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "# Create a dataframe with columns for software program, experience level, and completion time\n",
    "data = pd.DataFrame({\n",
    "    'Program': np.random.choice(['A', 'B', 'C'], size=90),\n",
    "    'Experience': np.random.choice(['Novice', 'Experienced'], size=90),\n",
    "    'Time': np.random.normal(loc=10, scale=2, size=90)  # Adjust mean and scale as needed\n",
    "})\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "formula = 'Time ~ C(Program) + C(Experience) + C(Program):C(Experience)'\n",
    "model = ols(formula, data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(\"ANOVA Table:\")\n",
    "print(anova_table)\n",
    "\n",
    "# Interpret the results\n",
    "print(\"\\nInterpretation:\")\n",
    "if anova_table['PR(>F)']['C(Program)'] < 0.05:\n",
    "    print(\"There is a significant main effect of software program on completion time.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of software program on completion time.\")\n",
    "\n",
    "if anova_table['PR(>F)']['C(Experience)'] < 0.05:\n",
    "    print(\"There is a significant main effect of experience level on completion time.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of experience level on completion time.\")\n",
    "\n",
    "if anova_table['PR(>F)']['C(Program):C(Experience)'] < 0.05:\n",
    "    print(\"There is a significant interaction effect between software program and experience level.\")\n",
    "else:\n",
    "    print(\"There is no significant interaction effect between software program and experience level.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19fabeb-6aa3-4564-b6b8-8f9ccddbc75d",
   "metadata": {},
   "source": [
    "Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81344f82-755b-4b7e-833b-4b789e5a9c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test results:\n",
      "T-statistic: -4.754695943505282\n",
      "P-value: 3.8246846694060195e-06\n",
      "There is a significant difference in test scores between the control and experimental groups.\n",
      "\n",
      "Post-hoc Test:\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "========================================================\n",
      " group1    group2    meandiff p-adj lower  upper  reject\n",
      "--------------------------------------------------------\n",
      "Control Experimental   6.2615   0.0 3.6645 8.8585   True\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "control_group = np.random.normal(loc=70, scale=10, size=100)\n",
    "experimental_group = np.random.normal(loc=75, scale=10, size=100)\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_statistic, p_value = ttest_ind(control_group, experimental_group, equal_var=False)\n",
    "\n",
    "# Print the results of the t-test\n",
    "print(f\"Two-sample t-test results:\")\n",
    "print(f\"T-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Check if the results are significant\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference in test scores between the control and experimental groups.\")\n",
    "    print(\"\\nPost-hoc Test:\")\n",
    "    \n",
    "    # Perform a post-hoc test (Tukey's HSD)\n",
    "    all_data = np.concatenate([control_group, experimental_group])\n",
    "    group_labels = ['Control'] * 100 + ['Experimental'] * 100\n",
    "    posthoc_results = MultiComparison(all_data, group_labels).tukeyhsd()\n",
    "\n",
    "    # Print the post-hoc results\n",
    "    print(posthoc_results)\n",
    "else:\n",
    "    print(\"There is no significant difference in test scores between the control and experimental groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092ac33-0f91-419d-af17-fbcab1452d7e",
   "metadata": {},
   "source": [
    "Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "on those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n",
    "\n",
    "significant differences in sales between the three stores. If the results are significant, follow up with a post-\n",
    "hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e5b710f-8c70-4949-bcae-1906d3a6166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data top 5 rows : \n",
      "   Day    Store        Sales\n",
      "0    0  Store A   933.187150\n",
      "1    1  Store A   950.179048\n",
      "2    2  Store A  1061.857582\n",
      "3    3  Store A  1056.869225\n",
      "4    4  Store A  1135.050948\n",
      "\n",
      "================================================\n",
      "\n",
      "               Anova\n",
      "===================================\n",
      "      F Value Num DF  Den DF Pr > F\n",
      "-----------------------------------\n",
      "Store 51.5040 2.0000 58.0000 0.0000\n",
      "===================================\n",
      "\n",
      "Reject the Null Hypothesis : Atleast one of the group has different mean.\n",
      "\n",
      "Tukey HSD posthoc test:\n",
      "    Multiple Comparison of Means - Tukey HSD, FWER=0.05    \n",
      "===========================================================\n",
      " group1  group2  meandiff p-adj    lower     upper   reject\n",
      "-----------------------------------------------------------\n",
      "Store A Store B   21.2439 0.6945   -40.881   83.3688  False\n",
      "Store A Store C -207.8078    0.0 -269.9328 -145.6829   True\n",
      "Store B Store C -229.0517    0.0 -291.1766 -166.9268   True\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# set random seed for reproducibility\n",
    "np.random.seed(456)\n",
    "\n",
    "# generate sales data for Store A, B, and C\n",
    "sales_a = np.random.normal(loc=1000, scale=100, size=(30,))\n",
    "sales_b = np.random.normal(loc=1050, scale=150, size=(30,))\n",
    "sales_c = np.random.normal(loc=800, scale=80, size=(30,))\n",
    "\n",
    "# create a DataFrame to store the sales data\n",
    "sales_df = pd.DataFrame({'Store A': sales_a, 'Store B': sales_b, 'Store C': sales_c})\n",
    "\n",
    "# reshape the DataFrame for repeated measures ANOVA\n",
    "sales_melted = pd.melt(sales_df.reset_index(), id_vars=['index'], value_vars=['Store A', 'Store B', 'Store C'])\n",
    "sales_melted.columns = ['Day', 'Store', 'Sales']\n",
    "\n",
    "# Printing top 5 rows of generated data\n",
    "print('Generated data top 5 rows : ')\n",
    "print(sales_melted.head())\n",
    "\n",
    "print('\\n================================================\\n')\n",
    "\n",
    "# perform repeated measures ANOVA\n",
    "rm_anova = AnovaRM(sales_melted, 'Sales', 'Day', within=['Store'])\n",
    "rm_results = rm_anova.fit()\n",
    "print(rm_results)\n",
    "\n",
    "# check if null hypothesis should be rejected based on p-value\n",
    "if rm_results.anova_table['Pr > F'][0] < 0.05:\n",
    "    # perform post-hoc Tukey test\n",
    "    print('Reject the Null Hypothesis : Atleast one of the group has different mean.\\n')\n",
    "    print('Tukey HSD posthoc test:')\n",
    "    tukey_results = pairwise_tukeyhsd(sales_melted['Sales'], sales_melted['Store'])\n",
    "    print(tukey_results)\n",
    "else:\n",
    "    print('NO significant difference between groups.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60cf114-075b-4d17-8282-47a1902548cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
